{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, I use Snowpark to load the full insurance dataset csv(1M rows) into a snowflake table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connection Established with the following parameters:\n",
      "User                        : SPORTY4992\n",
      "Role                        : \"ACCOUNTADMIN\"\n",
      "Database                    : \"INSURANCE\"\n",
      "Schema                      : \"ML_PIPE\"\n",
      "Warehouse                   : \"COMPUTE_WH\"\n"
     ]
    }
   ],
   "source": [
    "from snowflake.snowpark import Session\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Initiate session\n",
    "connection_parameters = json.load(open('./connection.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "snowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\n",
    "session.use_database('Insurance')\n",
    "session.use_schema('ML_PIPE')\n",
    "\n",
    "# Current Environment Details\n",
    "print('\\nConnection Established with the following parameters:')\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/sridharstreaks/insurance-data-for-machine-learning\n",
      "License(s): CC0-1.0\n",
      "Downloading insurance-data-for-machine-learning.zip to /Users/Tom/Documents/Snowflake/ML Pipeline/finished_version_on github/sf-train-inference-pipeline/create_locally\n",
      " 94%|███████████████████████████████████▊  | 20.0M/21.3M [00:01<00:00, 11.7MB/s]\n",
      "100%|██████████████████████████████████████| 21.3M/21.3M [00:01<00:00, 11.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d sridharstreaks/insurance-data-for-machine-learning --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full 1M dataset into dataframe\n",
    "insurance_df = pd.read_csv('insurance_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning, rearranging columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capitalize column names\n",
    "insurance_df.columns = insurance_df.columns.str.upper()\n",
    "\n",
    "# Rearrange columns to fit target schema\n",
    "cols = insurance_df.columns.tolist()\n",
    "cols = cols[:3] + cols[-1:] + cols[3:-1]\n",
    "insurance_df = insurance_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the write_pandas() method to write the first 10k rows into the 'SOURCE_OF_TRUTH' table created with the SQL commands in the SQL file. The method \"returns a Snowpark DataFrame object referring to the table where the pandas DataFrame was written to.\" (Snowpark Documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_of_truth_df = session.write_pandas(insurance_df[:10000], table_name='SOURCE_OF_TRUTH',database='INSURANCE',schema='ML_PIPE',auto_create_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below writes the remaining 990k to the INCOMING_DATA_SOURCE table to simulate data being streamed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tom/anaconda3/envs/sf-ml-pipeline/lib/python3.11/site-packages/snowflake/snowpark/session.py:2940: UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.range.RangeIndex'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)\n",
      "  success, _, _, ci_output = write_pandas(\n"
     ]
    }
   ],
   "source": [
    "incoming_data_source_df = session.write_pandas(insurance_df[10000:], table_name='INCOMING_DATA_SOURCE',database='INSURANCE',schema='ML_PIPE',auto_create_table=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf-ml-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
